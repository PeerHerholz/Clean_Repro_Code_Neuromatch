{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a04029-517b-41d6-9742-35956c2a097c",
   "metadata": {},
   "source": [
    "# Code testing\n",
    "\n",
    "<img align=\"center\" src=\"https://uploads-ssl.webflow.com/5f3c19f18169b62a0d0bf387/60d33beacf4ba7263a23cd79_qh6ImC4NPdyPbvn-7ns8FYsgOskDPDWLnX31mLCOgSwpX_SQgmo8krqdg4e6XAnSbqRAtZMYqlf7UTvlHiXgt5YtMwbt9IRY1fAbOjyq5hARui-xEQUgI48EOjhJGuIsSFDg90L6.jpeg\" alt=\"logo\" title=\"jupyter\" width=\"400\" height=\"700\" /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8f50e-bf82-46b4-b9d4-36a3db27ca31",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looking back at our [script](https://github.com/PeerHerholz/Clean_Repro_Code_Neuromatch/blob/main/workshop/materials/code_form_test_CI/emd_clust_cont_loss_sim.py) from the section before, ie the one you just brought up to code concerning `comments` and `formatting` based on `guidelines`, there is at least one other big question we have to address..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2503d-6154-4cc3-ad2f-98615f3d64c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we know that the `code` does what it is supposed to be doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358d1f1-fd3c-47a5-b663-fa213cb5777f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now you might think \"What do you mean? It is running the things I wrote down, done.\". However, the reality looks different..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420dabcc-6c68-4de3-9ddf-9398b9667b00",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "Generally, there are two major reasons why it is of the utmost importance to check and evaluate `code`:\n",
    "\n",
    "1. **Mistakes made while coding**\n",
    "2. **Code instability and changes**\n",
    "\n",
    "Let's have a quick look at both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2fe06a-7e3a-4000-900d-efef9893b6ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mistakes made while coding\n",
    "\n",
    "It is very, very easy to make mistakes when `coding`. A single misplaced character can cause a program/script’s output to be entirely wrong or vary tremendously from what its expected behavior. This can happen because a plus sign which should have been a minus or one piece of code working in one unit  while a piece of code written by another researcher worked in a differnt unit. Everyone makes mistakes, but the results can be catastrophic. Careers can be damaged/ended, vast sums of research funds can be wasted, and valuable time may be lost to exploring incorrect avenues. \n",
    "\n",
    "<img align=\"center\" src=\"https://the-turing-way.netlify.app/_images/testing-motivation1.png\" alt=\"logo\" title=\"jupyter\" width=\"600\" height=\"200\" /> \n",
    "<img align=\"center\" src=\"https://the-turing-way.netlify.app/_images/testing-motivation2.png\" alt=\"logo\" title=\"jupyter\" width=\"600\" height=\"200\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37500459",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img align=\"center\" src=\"https://gitlab.com/julia-pfarr/nowaschool/-/raw/main/school/materials/static/CI_CD/CI_CD_tests_imp_1.png?ref_type=heads\" alt=\"logo\" title=\"jupyter\" width=\"500\" height=\"200\" /> \n",
    "<img align=\"center\" src=\"https://gitlab.com/julia-pfarr/nowaschool/-/raw/main/school/materials/static/CI_CD/CI_CD_tests_imp_2.png?ref_type=heads\" alt=\"logo\" title=\"jupyter\" width=\"500\" height=\"100\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7a4d9-cbfe-4a5c-9995-874eb07d34f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code instabilities and changes\n",
    "\n",
    "The second reason is a also challening but in a different way...The `code` you are using and writing is affected by underlying `numerical instabilities` and `changes` during development. Just have a look at this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280e4f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000007\n"
     ]
    }
   ],
   "source": [
    "print(sum([0.001 for _ in range(1000)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61fa24-7e84-44c9-98b8-35464a6c6a3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Regarding the first, there are `intrinsic numerical erros & instabilities` that may lead `unstable functions` towards `distinct local minima`. This phenomenon is only aggravated by prominent differences between `OS`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd42bc-a699-49a8-85d4-0f3a887172c9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Concerning the second, a lot of the `code` you are using is going to be part of `packages` and `modules` that are `developed` and `maintained` by other people. Along this process, the `code`, e.g. a `function`, you are using is going to change, more or less prominently. It could be a `rounding change` or complete change of `inputs` and `outputs`. Either or, the effects on your application and/or pipeline might be significant and most importantely: unbeknownst to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9332c-e1a6-483b-bc46-44bcf8303fab",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is why `software` and `code tests` are vital! Ie, to ensure the expected outcome and check, as well as evaluate changes along the development process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a616b88-a279-43ee-95d7-8c9976256001",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd781a-8cb9-470c-9870-6cd18c152d61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### save time and effort\n",
    "\n",
    "Even if problems in a program are caught before research is published it can be difficult to figure out what results are contaminated and must be re-done. This represents a huge loss of time and effort. Catching these problems as early as possible minimises the amount of work it takes to fix them, and for most researchers time is by far their most scarce resource. You should not skip writing tests because you are short on time, you should write tests because you are short on time. Researchers cannot afford to have months or years of work go down the drain, and they can’t afford to repeatedly manually check every little detail of a program that might be hundreds or hundreds of thousands of lines long. Writing tests to do it for you is the time-saving option, and it’s the safe option.\n",
    "\n",
    "<img align=\"center\" src=\"https://media1.tenor.com/m/z67CJVOQTUcAAAAC/aint-nobody-got-time-no.gif\" alt=\"logo\" title=\"jupyter\" width=\"250\" height=\"150\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb404e-bf6e-462b-94d7-d6a7db3f0b6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### long-term confidence\n",
    "\n",
    "As researchers write code they generally do some tests as they go along, often by adding in print statements and checking the output. However, these tests are often thrown away as soon as they pass and are no longer present to check what they were intended to check. It is comparatively very little work to place these tests in functions and keep them so they can be run at any time in the future. The additional labour is minimal, the time saved and safeguards provided are invaluable. Further, by formalizing the testing process into a suite of tests that can be run independently and automatically, you provide a much greater degree of confidence that the software behaves correctly and increase the likelihood that defects will be found.\n",
    "\n",
    "<img align=\"center\" src=\"https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExZWhiaGx0aW13ejFvbTRjNWFiczMwNDV2dzlnbmQzcDd5a3hqOGEzMyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/TCUF10maUyDV6gguB7/giphy.gif\" alt=\"logo\" title=\"jupyter\" width=\"250\" height=\"250\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229d255-405c-4a59-a685-7927f21a0063",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### peace of mind\n",
    "\n",
    "Testing also affords researchers much more peace of mind when working on/improving a project. After changing their code a researcher will want to check that their changes or fixes have not broken anything. Providing researchers with a fail-fast environment allows the rapid identification of failures introduced by changes to the code. The alternative, of the researcher writing and running whatever small tests they have time for is far inferior to a good testing suite which can thoroughly check the code.\n",
    "\n",
    "<img align=\"center\" src=\"https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExMWxjODhxcjFiNGp5ZDFhanB2OXVpNW9pbGlnODlxdjE1eml6M2JmNiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/EzTyrBGeAmDSsToZxb/giphy.gif\" alt=\"logo\" title=\"jupyter\" width=\"250\" height=\"150\" /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa21b49-3d1c-46ab-857d-72f86d4bfe59",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### better code from the start\n",
    "\n",
    "Another benefit of writing tests is that it typically forces a researcher to write cleaner, more modular code as such code is far easier to write tests for, leading to an improvement in code quality. Good quality code is far easier (and altogether more pleasant) to work with than tangled rat’s nests of code I’m sure we’ve all come across (and, let’s be honest, written). This point is expanded upon in the section Unit Testing.\n",
    "\n",
    "<img align=\"center\" src=\"https://media1.tenor.com/m/DkF4Sll26-sAAAAd/this-is-the-place-to-start-right.gif\" alt=\"logo\" title=\"jupyter\" width=\"250\" height=\"150\" /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a506f8a-63e3-441e-96fe-02bafa8157d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Research software\n",
    "\n",
    "As well as advantaging individual researchers testing also benefits research as a whole. It makes research more reproducible by answering the question “how do we even know this code works”. If tests are never saved, just done and deleted the proof cannot be reproduced easily.\n",
    "\n",
    "Testing also helps prevent valuable grant money being spent on projects that may be partly or wholly flawed due to mistakes in the code. Worse, if mistakes are not at found and the work is published, any subsequent work that builds upon the project will be similarly flawed.\n",
    "\n",
    "Perhaps the cleanest expression of why testing is important for research as a whole can be found in the Software Sustainability Institute slogan: better software, better research.\n",
    "\n",
    "<img align=\"center\" src=\"https://gitlab.com/julia-pfarr/nowaschool/-/raw/main/school/materials/static/CI_CD/CI_CD_tests_imp_1.png?ref_type=heads\" alt=\"logo\" title=\"jupyter\" width=\"500\" height=\"200\" /> \n",
    "<img align=\"center\" src=\"https://gitlab.com/julia-pfarr/nowaschool/-/raw/main/school/materials/static/CI_CD/CI_CD_tests_imp_2.png?ref_type=heads\" alt=\"logo\" title=\"jupyter\" width=\"500\" height=\"100\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462329a-684d-46d4-ae6f-eb1ae5dd7c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Write Tests - Any Tests!\n",
    "\n",
    "Starting the process of writing `tests` can be overwhelming, especially if you have a large `code base`. Further to that, as mentioned, there are many kinds of `tests`, and implementing all of them can seem like an impossible mountain to climb. That is why the single most important piece of guidance in this chapter is as follows: write some `tests`. Testing one tiny thing in a `code` that’s thousands of lines long is infinitely better than `testing` nothing in a `code` that’s thousands of lines long. You may not be able to do everything, but doing something is valuable.\n",
    "\n",
    "Make improvements where you can, and do your best to include `tests` with new `code` you write even if it’s not feasible to write `tests` for all the `code` that’s already written.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/Clean_Repro_Code_Neuromatch/refs/heads/main/workshop/materials/static/CI_CD/tests_write.png\" alt=\"logo\" title=\"jupyter\" width=\"900\" height=\"400\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcccfd7-19fd-416c-ba16-7e61b4a8f786",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Run the tests\n",
    "\n",
    "The second most important piece of advice in this chapter: `run` the `tests`. Having a beautiful, perfect `test suite` is no use if you rarely run it. Leaving long gaps between `test runs` makes it more difficult to track down what has gone wrong when a `test` `fails` because, a lot of the `code` will have changed. Also, if it has been weeks or months since `tests` have been run and they fail, it is difficult or impossible to know which results that have been obtained in the mean time are still valid, and which have to be thrown away as they could have been impacted by the `bug`.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/Clean_Repro_Code_Neuromatch/refs/heads/main/workshop/materials/static/CI_CD/tests_run.png\" alt=\"logo\" title=\"jupyter\" width=\"900\" height=\"400\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1de51-1e39-4d23-b582-5e8e6e663a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is best to `automate` your `testing` as far as possible. If each `test` needs to be run individually then that boring painstaking process is likely to get neglected. This can be done by making use of a `testing framework` (discussed later). Ideally set your `tests` up to run at regular intervals, possibly every night."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bac2cd-a49a-4568-a1ac-674bdd9ce79b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider setting up `continuous integration` (discussed in the [continuous integration sesssion](https://peerherholz.github.io/Clean_Repro_Code_Neuromatch/materials/code_form_test_CI/CI_CD.html)) on your project. This will automatically `run` your `tests` each time you make a change to your `code` and, depending on the `continuous integration` software you use, will notify you if any of the `tests` `fail`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524d573-634b-4d03-8471-d9f20eedee99",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Consider how long it takes your tests to run\n",
    "\n",
    "Some tests, like `Unit Testing` only `test` a small piece of `code` and so typically are very fast. However other kinds of `tests`, such as `System Testing` which `test` the entire `code` from end to end, may take a long time to run depending on the `code`. As such it can be obstructive to run the entire `test suite` after each little bit of work.\n",
    "\n",
    "<img align=\"center\" src=\"https://media.tenor.com/sArqcAPZZjAAAAAM/robin-williams.gif\" alt=\"logo\" title=\"jupyter\" width=\"250\" height=\"150\" /> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87563e5-2fbd-410c-9bf5-2965df29980e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In that case it is better to run lighter weight `tests` such as `unit tests` frequently, and longer `tests` only once per day, overnight. It is also good to scale the number of each kind of `tests` you have in relation to how long they take to run. You should have a lot of `unit tests` (or other types of `tests` that are fast) but much fewer `tests` which take a long time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339510e-befd-4b98-aa2e-8207248a49f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Document the tests and how to run them\n",
    "\n",
    "It is important to provide documentation that describes how to run the `tests`, both for yourself in case you come back to a project in the future, and for anyone else that may wish to build upon or reproduce your work. \n",
    "\n",
    "<img align=\"center\" src=\"https://uploads-ssl.webflow.com/5f3c19f18169b62a0d0bf387/60d33be8cf4ba7565123c8bc_YPD3ulQQAGQpOcnqIm3QzSTRgzmr1SexpW9ZjMpJ1mAnUxx4iF05XOTu44sk0qQG-8XgBcYmGZGAD-5SAZvJl3TjtmhgWnn-w0C2XKwhBscV78RVvhwZfyp0v_Pa6sNj5zxpOvRW.png\" alt=\"logo\" title=\"jupyter\" width=\"250\" height=\"250\" /> \n",
    "\n",
    "\n",
    "\n",
    "This documentation should also cover subjects such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5710762-533a-4617-b5f6-af6e4a408ab8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- any resources, such as `test dataset` files that are required\n",
    "\n",
    "- any `configuration`/`settings` adjustments needed to run the tests\n",
    "\n",
    "- what `software` (such as `testing frameworks`) need to be installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca105c-6668-4b09-82c3-45fe7ae3a8ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ideally, you would provide scripts to set up and configure any resources that are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2773db-dd3d-4b1c-95da-ebd888448ced",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test Realistic Cases\n",
    "\n",
    "Make the cases you `test` as realistic as possible. If for example, you have `dummy data` to run `tests` on you should make sure that `data` is as similar as possible to the actual `data`. If your actual `data` is messy with a lot of `null values`, so should your `test dataset` be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78c5d3-ef0e-447a-ba64-a5edc0d3dee9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Use a Testing Framework\n",
    "\n",
    "There are `tools` available to make `writing` and `running` `tests` easier, these are known as `testing frameworks`. Find one you like, learn about the features it offers, and make use of them. A very `common testing framework` for `python` is [pytest](https://docs.pytest.org/en/8.0.x/).\n",
    "\n",
    "\n",
    "<iframe src=\"https://docs.pytest.org/en/stable/\" width=\"650\" height=\"500\"></iframe>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a905d-a7a9-43b1-9aee-acc701f23f19",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coverage\n",
    "\n",
    "`Code coverage` is a measure of how much of your code is “`covered`” by `tests`. More precisely it a measure of how much of your `code` is run when `tests` are conducted. So for example, if you have an `if statement` but only `test` things where that `if statement` evaluates to “`False`” then none of the `code` in the `if block` will be run. As a result your `code coverage` would be `< 100%`. `Code coverage` doesn’t include documentation like `comments`, so adding more documentation doesn’t affect your percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90398326-d842-4628-9ffb-078d58a59a2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`Code coverage` gauges how much `code` your `tests` run, aiming for as close to `100%` as possible without counting documentation. High coverage is ideal, though any `testing` is beneficial. Various `tools` and `bots` measure this across `programming languages`, e.g. `pytest` for `python`. Beware the illusion of good `coverage`; thorough `testing` involves multiple scenarios for the same `code`, emphasizing `testing` smaller `code` chunks for precise logic validation. `Testing` the same code multiple ways is encouraged for comprehensive assessment.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/Clean_Repro_Code_Neuromatch/refs/heads/main/workshop/materials/static/CI_CD/test_coverage.png\" alt=\"logo\" title=\"jupyter\" width=\"900\" height=\"400\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c286bc-7d92-428d-99be-e846397461d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Use test doubles/stubs/mocking where appropriate\n",
    "\n",
    "Use `test doubles` like `stubs` or `mocks` for isolating `code` in `tests`. Ensure `tests` make it easy to pinpoint failures, which can be hard when `code` depends on external factors like `internet connections` or `objects`. For example, a `web interaction test` might `fail` due to internet issues, not `code bugs`. Similarly, a `test` involving an `object` might `fail` because of the `object` itself, which should have its own `tests`. Eliminate these dependencies with `test doubles`, which come in several types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ece3e-248c-4c78-97b4-9cc3364a0c78",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `Dummy objects` are `placeholders` that aren’t actually used in `testing` beyond filling method parameters.\n",
    "\n",
    "    ```python\n",
    "    # Dummy value to fill in as a placeholder\n",
    "    dummy_logger = None\n",
    "\n",
    "    def process_data(data, logger):\n",
    "        # The function doesn't actually use the logger in this case\n",
    "        return f\"Processed {data}\"\n",
    "\n",
    "    print(process_data(\"input_data\", dummy_logger))  # Output: Processed input_data\n",
    "    ```\n",
    "\n",
    "- `Fake objects` have simplified, functional implementations, like an `in-memory database` instead of a real one.\n",
    "\n",
    "    ```python\n",
    "    # Fake buffer \n",
    "    fake_buffer = []\n",
    "\n",
    "    def add_reading(buffer, reading):\n",
    "        buffer.append(reading)\n",
    "\n",
    "    def get_latest_reading(buffer):\n",
    "        return buffer[-1] if buffer else None\n",
    "\n",
    "    # Using the fake buffer in a test\n",
    "    add_reading(fake_buffer, 25.3)\n",
    "    print(get_latest_reading(fake_buffer))  # Output: 25.3\n",
    "    ```\n",
    "\n",
    "- `Stubs` provide partial implementations to respond only to specific `test` cases and might record call information.\n",
    "\n",
    "    ```python\n",
    "    # Stub that only provides results for specific test inputs\n",
    "    stub_calls = []  # List to record calls\n",
    "\n",
    "    def calculate_distance(speed, time):\n",
    "        stub_calls.append((speed, time))  # Record the call\n",
    "        if speed == 10 and time == 2:\n",
    "            return 20  # A specific expected response\n",
    "        return None\n",
    "\n",
    "    # Using the stub in a test\n",
    "    print(calculate_distance(10, 2))  # Output: 20\n",
    "    print(stub_calls)                 # Output: [(10, 2)]\n",
    "    ```\n",
    "\n",
    "- `Mocks` simulate `interfaces` or `classes`, with `predefined outputs` for `method` calls, often recording interactions for `test` validation.\n",
    "\n",
    "    ```python\n",
    "    from unittest.mock import Mock\n",
    "\n",
    "    # Mock function to simulate a complex calculation\n",
    "    mock_calculate = Mock(return_value=42)\n",
    "\n",
    "    # Using the mock in a test\n",
    "    result = mock_calculate(5, 7)\n",
    "    print(result)  # Output: 42\n",
    "\n",
    "    # Verify the mock was called with specific arguments\n",
    "    mock_calculate.assert_called_with(5, 7)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf2838-9bf2-4707-82a6-d50fe4c64dff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`Test doubles` replace real dependencies, making `tests` more focused and reliable. `Mocks` can be hand-coded or generated with `mock frameworks`, which allow dynamic behavior definition. A common `mock example` is a `data` provider, where a `mock` simulates the `data source` to ensure consistent `test conditions`, contrasting with the real `data source` used in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d9cbf-d788-47f3-b1cd-e344cce9d3a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(testing-basics)=\n",
    "## Code Testing - the basics\n",
    "\n",
    "There are a number of different kinds of `tests`, which will be briefly discussed in the follwing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a268d-64a9-41ca-8af7-0b9fcf0b495b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Firstly, there are `positive tests` and `negative tests`. `Positive tests` check that something works, for example, `testing` that a `function` that multiplies some numbers together outputs the correct answer. `Negative tests` check that something generates an `error` when it should. For example, nothing can go quicker than the speed of light, so a plasma physics simulation `code` may contain a test that an error is outputted if there are any particles faster than this, as it indicates there is a deeper problem in the `code`.\n",
    "\n",
    "<img align=\"center\" src=\"https://www.techtarget.com/rms/onlineImages/software_quality-postive_vs_negative_test_cases-f.png\" alt=\"logo\" title=\"jupyter\" width=\"800\" height=\"400\" /> \n",
    "\n",
    "Image credit: [TechTarget](https://www.techtarget.com/rms/onlineImages/software_quality-postive_vs_negative_test_cases-f.png).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8603864a-80a7-43c7-b1c6-e8a5ccc19993",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition to these two kinds of `tests`, there are also different levels of `tests` which `test` different aspects of a project. These levels are outlined below and both `positive` and `negative tests` can be present at any of these `levels`. A thorough `test suite` will contain `tests` at all of these `levels` (though some `levels` will need very few).\n",
    "\n",
    "<img align=\"center\" src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc53f283e-6834-45b6-beb9-57b10b2a22fd_1280x1664.gif\" alt=\"logo\" title=\"jupyter\" width=\"500\" height=\"800\" /> \n",
    "\n",
    "Image credit: [ByteByteGo](https://blog.bytebytego.com/p/ep83-explaining-9-types-of-api-testing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0865880-c033-4c52-a7d0-4b2e83f65537",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, before we will check out the different `test` options, we have to talk about one aspect that is central to all: `assert`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a0d05-3c7e-4845-9f95-af4945e53051",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Assert - a test's best friend\n",
    "\n",
    "In order to check and evaluate if a certain piece of `code` is doing what it is supposed to do, in a reliable manner, we need a way of `assert`ing what the \"correct output\" should be and `test`ing the outcome we get against it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa199df-4045-4dee-84ab-ae6cf49032c0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In `Python`, `assert` is a `statement` used to `test` whether a `condition` is `true`. If the `condition` is `true`, the program continues to execute as normal. If the `condition` is `false`, the program raises an `AssertionError` exception and optionally can display an accompanying message. The primary use of `assert` is for `debugging` and `testing` purposes, where it helps to catch `errors` early by ensuring that certain `conditions` hold at specific points in the `code`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba26ac4-0527-499f-9e0d-0f082510151f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Syntax\n",
    "\n",
    "The basic `syntax` of an `assert statement` is:\n",
    "\n",
    "```python\n",
    "assert condition, \"Optional error message\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95c2b4-5d5f-47d8-9216-beb8f7d726fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `condition`: This is the expression to be `tested`. If the `condition` evaluates to `True`, nothing happens, and the program continues to execute. If it evaluates to `False`, an `AssertionError` is raised.\n",
    "  \n",
    "- `\"Optional error message\"`: This is the message that is shown when the `condition` is `false`. This message is optional, but it's helpful for understanding why the `assertion` `failed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de665a2a",
   "metadata": {},
   "source": [
    "#### A simple example\n",
    "\n",
    "Here's a simple example demonstrating how `assert` might be used in a `test case`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13d0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b \n",
    "\n",
    "# Test case for the add function\n",
    "def test_add():\n",
    "    result = add(2, 3) \n",
    "    assert result == 5, \"Expected add(2, 3) to be 5\"\n",
    "\n",
    "test_add()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81c9d9",
   "metadata": {},
   "source": [
    "If `add(2, 3)` did not `return 5`, the `assert statement` raises an `AssertionError` with the message `\"Expected add(2, 3) to be 5\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011724d-5cb0-4c74-b556-a799a0ae94af",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Usage in Testing\n",
    "\n",
    "In the context of `testing`, `assert statements` are used to verify that a `function` or a piece of `code` behaves as expected. They are a simple yet powerful tool for writing `test cases`, where you check the `outcomes` of various `functions` under different `inputs`. Here’s how you might use `assert` in a `test`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf0777-0c40-4622-89ed-677aa3359261",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Checking Function Outputs`: To verify that a `function` returns the expected `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fb0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Assert that the function output is as expected\n",
    "result = add(3, 4)  \n",
    "assert result == 7, f\"Expected 7, but got {result}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33cd92d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected 7, but got 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m add(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)  \n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m7\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 7, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected 7, but got 8"
     ]
    }
   ],
   "source": [
    "result = add(4, 4)  \n",
    "assert result == 7, f\"Expected 7, but got {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9b9ba",
   "metadata": {},
   "source": [
    "- `Validating Data Types`: To ensure that `variables` or `return values` are of the `correct type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca63febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(name):\n",
    "    return name\n",
    "\n",
    "# Assert that the return value is a string\n",
    "name = get_name(\"Alice\")\n",
    "assert isinstance(name, str), f\"Expected type str, but got {type(name).__name__}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb4d69f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected type str, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m name \u001b[38;5;241m=\u001b[39m get_name(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected type str, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(name)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected type str, but got int"
     ]
    }
   ],
   "source": [
    "name = get_name(1)\n",
    "assert isinstance(name, str), f\"Expected type str, but got {type(name).__name__}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721021e3",
   "metadata": {},
   "source": [
    "- `Testing Invariants`: To check `conditions` that should always be `true` in a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae18926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_list(item, lst):\n",
    "    lst=[]\n",
    "    lst.append(item)\n",
    "    return lst\n",
    "\n",
    "# Testing the invariant: the list should not be empty after adding an item\n",
    "items = add_to_list(\"apple\", [])\n",
    "assert items, \"List should not be empty after adding an item\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf983f",
   "metadata": {},
   "source": [
    "- `Comparing Data Structures`: To ensure that `lists`, `dictionaries`, `sets`, etc., contain the expected `elements`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da3d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_list = [1, 2, 3]\n",
    "expected_list = [1, 2, 3]\n",
    "\n",
    "# Assert that both lists are identical\n",
    "assert actual_list == expected_list, f\"Expected {expected_list}, but got {actual_list}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1611157f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected [1, 2, 3], but got [1, 2, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m expected_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assert that both lists are identical\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m actual_list \u001b[38;5;241m==\u001b[39m expected_list, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected [1, 2, 3], but got [1, 2, 4]"
     ]
    }
   ],
   "source": [
    "actual_list = [1, 2, 4]\n",
    "expected_list = [1, 2, 3]\n",
    "\n",
    "# Assert that both lists are identical\n",
    "assert actual_list == expected_list, f\"Expected {expected_list}, but got {actual_list}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da455a27-8428-428d-9273-815d87272d26",
   "metadata": {},
   "source": [
    "```{admonition} How could you \"break\" the test?\n",
    ":class: tip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972845c-e920-435b-abde-6f47c1f0c3b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Best Practices\n",
    "\n",
    "- `Use for Testing`: Leverage `assert` primarily in `testing frameworks` or during the `debugging phase`, not as a mechanism for handling `runtime errors` in production `code`.\n",
    "- `Clear Messages`: Include `clear`, `descriptive messages` with `assert statements` to make it easier to identify the cause of a `test` `failure`.\n",
    "- `Test Precisely`: Each `assert` should `test` one specific aspect of your `code`'s behavior to make diagnosing issues straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a42a1-872f-48f5-9633-2d1ce31744c9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Runtime testing\n",
    "\n",
    "`Runtime tests` are `tests` that run as part of the program itself. They may take the form of checks within the `code`, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe1ca1-fca7-432f-b827-cea0a9aee804",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, we could use the following `runtime tests` to `test` the first block of our [emd_clust_cont_loss_sim.py script](https://github.com/PeerHerholz/Clean_Repro_Code_Neuromatch/blob/main/workshop/materials/code_form_test_CI/emd_clust_cont_loss_sim.py): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320e9a0f-8acd-468b-9aad-5efbe29f1eb6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/ does not exist, creating it.\n",
      "./outputs/data does not exist, creating it.\n",
      "data shape as expected: (2000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary modules and functions\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define paths for all to be generated outputs\n",
    "output_path = \"./outputs/\"\n",
    "output_path_data = \"./outputs/data\"\n",
    "\n",
    "\n",
    "# For all output paths it should be checked if\n",
    "# they exist and if not, they should be created\n",
    "list_paths = [output_path, output_path_data]\n",
    "\n",
    "# Loop over the list of paths that should be checked\n",
    "for outputs in list_paths:\n",
    "\n",
    "    # if path exists, all good\n",
    "    if os.path.isdir(outputs):\n",
    "        print(f\"{outputs} exists.\")\n",
    "    # if not, path should be created\n",
    "    else:\n",
    "        print(f\"{outputs} does not exist, creating it.\")\n",
    "        os.makedirs(outputs)\n",
    "\n",
    "\n",
    "# Define seeds to address randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create, check and save simulated data\n",
    "data = np.random.randn(2000, 100)\n",
    "\n",
    "# Check data shape and if incorrect, raise error\n",
    "if data.shape==(2000, 100):\n",
    "    print('data shape as expected: %s' %str(data.shape))\n",
    "else:\n",
    "    print('data shape should be (2000, 100) but got: %s' % str(data.shape))\n",
    "    raise RuntimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b5438-538f-4599-80ae-913ba63f3bbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Advantages of runtime testing:\n",
    "\n",
    "- `run` within the program, so can catch problems caused by logic errors or edge cases\n",
    "\n",
    "- makes it easier to find the cause of the `bug` by catching problems early\n",
    "\n",
    "- catching problems early also helps prevent them escalating into catastrophic failures. It minimises the blast radius."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c2c8a-cfd5-4295-a279-db22c5b3681c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Disadvantages of runtime testing:\n",
    "\n",
    "- `tests` can slow down the program\n",
    "\n",
    "- what is the right thing to do if an `error` is detected? How should this `error` be reported? Exceptions are a recommended route to go with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55cf7b",
   "metadata": {},
   "source": [
    "(testing-frameworks)=\n",
    "## Testing frameworks\n",
    "\n",
    "Before if continue to explore different types of testing, we need to talk about `testing frameworks` as\n",
    "they are essential in the `software` development process, enabling developers and researchers to ensure their `code` behaves as expected. These `frameworks` facilitate various types of `testing`, such as `unit testing`, `integration testing`, `functional testing`, `regression testing`, and `performance testing`. By `automating` the execution of `tests`, verifying `outcomes`, and `reporting results`, `testing frameworks` help improve `code quality` and `software stability`.\n",
    "\n",
    "Importantly, you should start utilizing them right away and don't switch to a `testing framework` later on in your project. While you could of course do that, using them from the get go will save you a lot of time and help catch/prevent errors immediately. That's why we talk about them before addressing different test types so that we can showcase the latter in a test framework for best practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b35a4",
   "metadata": {},
   "source": [
    "(testing-frameworks-features)=\n",
    "### Key Features of Testing Frameworks\n",
    "\n",
    "- **Test Organization**: Helps structure and manage tests effectively.\n",
    "- **Fixture Management**: Supports setup and teardown operations for tests.\n",
    "- **Assertion Support**: Provides tools for verifying test outcomes.\n",
    "- **Automated Test Discovery**: Automatically identifies and runs tests.\n",
    "- **Mocking and Patching**: Allows isolation of the system under test.\n",
    "- **Parallel Test Execution**: Reduces test suite execution time.\n",
    "- **Extensibility**: Offers customization through plugins and hooks.\n",
    "- **Reporting**: Generates detailed reports on test outcomes.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4eaf1",
   "metadata": {},
   "source": [
    "(testing-frameworks-setup)=\n",
    "### Setup for using testing frameworks\n",
    "\n",
    "While you could add tests directly to your `code`, we recommend refactoring and reorganizing your project setup slightly to keep everything `organized`, `testable`, and `maintainable` without duplicating `code` between your `analysis` and `test scripts`.\n",
    "\n",
    "In general, this entails the following aspects:\n",
    "\n",
    "- have separate scripts for the `code`/`analyses` and its respective `tests` \n",
    "- write `functions` for parts of your `code` that run certain tasks/elements (e.g. `data downloading`, `preprocessing`, `analyses`)\n",
    "- `import` and `test` these `functions` in the `test scripts`\n",
    "\n",
    "With that, you have one \"truth\" of the `code`/`analyses` and can automatically `test` different parts of them using the `testing framework`.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/Clean_Repro_Code_Neuromatch/refs/heads/main/workshop/materials/static/CI_CD/tests_setup.png\" alt=\"logo\" title=\"jupyter\" width=\"900\" height=\"400\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc749ad9",
   "metadata": {},
   "source": [
    "There are many different testing frameworks for `python`, including [behave](https://behave.readthedocs.io/en/latest/), [Robot Framework](https://robotframework.org/) and [TestProject](https://github.com/testproject-io). However, we will focus on [pytest](https://docs.pytest.org/en/stable/).\n",
    "\n",
    "### `pytest`\n",
    "\n",
    "[pytest](https://docs.pytest.org/en/stable/) is a powerful `testing framework` for `Python` that is easy to start with but also supports complex functional `testing`. It is known for its simple `syntax`, detailed `assertion` introspection, `automatic test` discovery, and a wide range of `plugins` and `integrations`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31a159",
   "metadata": {},
   "source": [
    "#### Running pytest\n",
    "\n",
    "There are various options to run `pytest`. Let's start with the easiest one, running all `tests` written in a specific `test directory`, ie the `setup` we introduce above.  \n",
    "At first, you need to ensure `pytest` is installed in your `computational environment`. If not, install it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbc532ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages (8.3.3)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages (from pytest) (1.5.0)\n",
      "Requirement already satisfied: iniconfig in /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages (from pytest) (2.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages (from pytest) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1 in /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages (from pytest) (2.0.1)\n",
      "Requirement already satisfied: packaging in /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages (from pytest) (23.2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591696b4",
   "metadata": {},
   "source": [
    "Additionally, you have to make sure that all your `tests` are placed in a dedicated `directory` and that their filenames follows one of these patterns: `test_*.py` or `*_test.py`.  \n",
    "\n",
    "they should ideally be placed in a dedicated directory that is different from the path where you work on the `code`/`analysis scripts`. Thus, let's create a respective `test directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e744ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('./tests', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5f0fc",
   "metadata": {},
   "source": [
    "Next, we will create our `test` files and save them in the `test` `directory`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504ff88",
   "metadata": {},
   "source": [
    "#### Writing Basic Tests with `pytest`\n",
    "\n",
    "Tests in `pytest` are simple to write. Starting with test functions, as tests grow, `pytest` provides a rich set of features for more complex scenarios. The crucial parts as mentioned before are: `assert` and using `functions`. You can write one `test file` per `test` or have multiple `tests` in a single `test file` based on their intended behavior/function.\n",
    "\n",
    "We will start with a very simple example of the former.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dbeb836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/test_addition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_addition.py\n",
    "\n",
    "\n",
    "def test_example():\n",
    "    assert 1 + 1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c5362",
   "metadata": {},
   "source": [
    "We can now run `pytest` by pointing it to the `directory` where we created and saved the `test` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caabd655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[32m                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c7539",
   "metadata": {},
   "source": [
    "`pytest` will automatically discover `tests` within any `files` that match the `pattern` described above in the `directory` and its `subdirectories`. As you can see above, it provides a comprehensive output report with details about the `computational environment` (used `software` and its `version`), as well as how many tests were run, their coverage of the `code` and if they resulted in `pass` or `fail`.\n",
    "\n",
    "Using this setup it's easy to run `automated tests` whenever you change something in your `code`/`analyses` and evaluate if that had an effect on the expected `behavior`/`function` of the `code`.\n",
    "\n",
    "Let's check a few more things about `pytest` that will come in handy when working with complex `scripts`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1716b",
   "metadata": {},
   "source": [
    "#### Using Fixtures for Setup and Teardown\n",
    "\n",
    "`pytest` fixtures define setup and teardown logic for `tests`, ensuring `tests` `run` under `controlled conditions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "335557d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/test_fixture.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_fixture.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "@pytest.mark.sum\n",
    "def test_sum(sample_data):\n",
    "    assert sum(sample_data) == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9499e",
   "metadata": {},
   "source": [
    "#### Parameterizing Tests\n",
    "\n",
    "`pytest` allows running a single `test` function with different `inputs` using `@pytest.mark.parametrize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06abbbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/test_parameterized.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_parameterized.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"a,b,expected\", [(1, 1, 2), (2, 3, 5), (3, 3, 6)])\n",
    "def test_addition(a, b, expected):\n",
    "    assert a + b == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aeb137",
   "metadata": {},
   "source": [
    "We can now run all `tests` by pointing `pytest` to the respective `directory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56799831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 20%]\u001b[0m\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[32m                                   [ 40%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 60%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[2-3-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 80%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[3-3-6] \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.16s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca09fa5",
   "metadata": {},
   "source": [
    "or run specific `tests`, e.g. `tests` from a specific `file` or those matching a certain `pattern`.  \n",
    "You can specify the `file` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc03f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[32m                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest tests/test_fixture.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521ee6d",
   "metadata": {},
   "source": [
    "or `run` `tests` matching a name pattern like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2333a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items / 1 deselected / 4 selected\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 25%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 50%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[2-3-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 75%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[3-3-6] \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest tests/ -k \"test_addition\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3e84d",
   "metadata": {},
   "source": [
    "You can also `run` `tests` marked with a `Custom Marker`: If you've used `custom markers` to decorate your `tests` (e.g., `@pytest.mark.regression`), you can `run` only the `tests` with that `marker`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6febb469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items / 4 deselected / 1 selected\n",
      "\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[32m                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m4 deselected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest tests/ -m \"sum\" -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9896699",
   "metadata": {},
   "source": [
    "NB: we do get a `warning` related to an `unregistered marker` as we didn't create a `pytest.ini` file yet.\n",
    "The `pytest.ini file` is a `configuration file` for `pytest` that allows you to specify `settings`, register custom `markers`, `configure test paths`, and more. It is typically placed in the `root directory` of your `project` and helps streamline the `pytest setup` by keeping `configurations` in one place.\n",
    "\n",
    "Thus, let's create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d75e4b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pytest.ini\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pytest.ini\n",
    "\n",
    "[pytest]\n",
    "# Register custom markers to avoid warnings\n",
    "markers =\n",
    "    sum: mark a test as related to summation operations\n",
    "\n",
    "# Default command-line options\n",
    "addopts = -v\n",
    "\n",
    "# Specify default test directories\n",
    "testpaths = ./tests\n",
    "\n",
    "# Specify the path for the source files\n",
    "pythonpath = ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e5846",
   "metadata": {},
   "source": [
    "With that in place, we can now run `pytest` with even less effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2bf293d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "testpaths: ./tests\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 5 items\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 20%]\u001b[0m\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[32m                                   [ 40%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 60%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[2-3-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 80%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[3-3-6] \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67849ed1",
   "metadata": {},
   "source": [
    "After learning about the basics of `testing frameworks`, it's time to check how we can use them to implement different `test types` by also incorporating the `setup` we talked about before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c20266",
   "metadata": {},
   "source": [
    "## Code testing - different types of tests\n",
    "\n",
    "We already explored different types of tests a little bit in the [above section](testing-basics) but it's time to go more into detail and learned more about common `test types` by using them for our `code`/`analyses`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fa178-a16f-44c8-93b5-5871c4e22c5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Smoke tests\n",
    "\n",
    "These are very brief initial checks that ensures the basic requirements required to run the `code` hold. If these `fail` there is no point proceeding to additional levels of `testing` until they are fixed. \n",
    "\n",
    "One example for our `code`/`analyses` would be to check if the `directories` in the `output paths` are created, as without them, we wouldn't get our `outputs` saved at all or somewhere else.\n",
    "\n",
    "As mentioned before, to follow best practices and make things manageable and easy for the long run, we need to refractor our `code`/`analyses` to run certain parts within `functions` that we can `import` in the `tests`.\n",
    "\n",
    "For the `output directory generation` part, we would change things from:\n",
    "\n",
    "```python\n",
    "# Import all necessary modules and functions\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define paths for all to be generated outputs\n",
    "output_path = \"./outputs/\"\n",
    "output_path_data = \"./outputs/data\"\n",
    "output_path_graphics = \"./outputs/graphics\"\n",
    "output_path_models = \"./outputs/models\"\n",
    "\n",
    "# For all output paths it should be checked if\n",
    "# they exist and if not, they should be created\n",
    "list_paths = [output_path, output_path_data, output_path_graphics,\n",
    "              output_path_models]\n",
    "\n",
    "# Loop over the list of paths that should be checked\n",
    "for outputs in list_paths:\n",
    "\n",
    "    # if path exists, all good\n",
    "    if os.path.isdir(outputs):\n",
    "        print(f\"{outputs} exists.\")\n",
    "    # if not, path should be created\n",
    "    else:\n",
    "        print(f\"{outputs} does not exist, creating it.\")\n",
    "        os.makedirs(outputs)\n",
    "```\n",
    "\n",
    "to \n",
    "\n",
    "```python\n",
    "# Import all necessary modules\n",
    "import os\n",
    "\n",
    "# Define utility function to create paths\n",
    "def create_output_paths(paths):\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    return paths\n",
    "\n",
    "# Run all steps of the code/analysis\n",
    "def main():\n",
    "    # Set up output directories\n",
    "    paths = [\"./outputs/\", \"./outputs/data\", \"./outputs/graphics\", \"./outputs/models\"]\n",
    "    create_output_paths(paths)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61f2eb",
   "metadata": {},
   "source": [
    "So, let's create a respective version of our `code`/`analyses` to make this possible. It's not best practice but for now we will create a new file instead of updating the old one so that we can easily compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7c2b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./emd_clust_cont_loss_sim_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./emd_clust_cont_loss_sim_functions.py\n",
    "\n",
    "# Import all necessary modules\n",
    "import os\n",
    "\n",
    "# Define utility function to create paths\n",
    "def create_output_paths(paths):\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    return paths\n",
    "\n",
    "# Define test directories\n",
    "paths = [\"./outputs/\", \"./outputs/data\", \"./outputs/graphics\", \"./outputs/models\"]\n",
    "\n",
    "\n",
    "# Run all steps of the code/analysis\n",
    "def main():\n",
    "    # Set up output directories\n",
    "    create_output_paths(paths)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e283c31",
   "metadata": {},
   "source": [
    "We can now setup a `smoke test` to evaluate if the `output paths` are generated as expected by utilizing, ie `import`ing, the just defined `function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6884652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/test_paths.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_paths.py\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pytest\n",
    "from emd_clust_cont_loss_sim_functions import create_output_paths \n",
    "\n",
    "# Define test paths (use temporary or unique paths to avoid overwriting real data)\n",
    "test_paths = [\"./test_outputs/\", \"./test_outputs/data\", \"./test_outputs/graphics\", \"./test_outputs/models\"]\n",
    "\n",
    "# Define tests\n",
    "def test_create_output_paths():\n",
    "\n",
    "    # Run the path creation function\n",
    "    create_output_paths(test_paths)\n",
    "\n",
    "    # Check if each path now exists\n",
    "    for path in test_paths:\n",
    "        assert os.path.isdir(path), f\"Expected directory {path} to be created, but it was not.\"\n",
    "\n",
    "    # Cleanup - remove the directories after the test to keep the environment clean\n",
    "    for path in reversed(test_paths):  # Remove subdirectories before parent directory\n",
    "        if os.path.isdir(path):\n",
    "            os.rmdir(path)\n",
    "\n",
    "# Cleanup - remove generated outputs to keep the test environment clean\n",
    "for path in reversed(test_paths):\n",
    "        if os.path.isdir(path):\n",
    "            shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3383e",
   "metadata": {},
   "source": [
    "With everything in place, we can now run `pytest` as we did before, pointing it to the specific `test` that we would like to `run`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "878c44e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_paths.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[32m                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest tests/test_paths.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17de9c20",
   "metadata": {},
   "source": [
    "That appears to work out, great! We could also run all `tests` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68646906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "testpaths: ./tests\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 6 items\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 16%]\u001b[0m\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[32m                                   [ 33%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 50%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[2-3-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 66%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[3-3-6] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 83%]\u001b[0m\n",
      "tests/test_paths.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[32m                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9b034",
   "metadata": {},
   "source": [
    "Isn't that great? Let's continue with exploring and adding more `test types`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d53dc-de76-4f82-a936-6ce0b03bb10f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unit tests\n",
    "\n",
    "A level of the `software testing` process where individual `units` of a `software` are `tested`. The purpose is to validate that each `unit` of the `software` performs as designed. For example, in our `code`/`analyses`, we could evaluate if the `functions` for `path` and `data` generation work as expected. Thus, we could write a `unit test` for both comparing the expected to the actual output.\n",
    "\n",
    "At first, we need to refactor the `data generation` in our `code`/`analyses` to be run as a `function`, ie adding it to the new version of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "695ceed1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./emd_clust_cont_loss_sim_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./emd_clust_cont_loss_sim_functions.py\n",
    "\n",
    "# Import all necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define utility function to create paths\n",
    "def create_output_paths(paths):\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    return paths\n",
    "\n",
    "# Define utility function to generate data\n",
    "def simulate_data(seed=42, size=(2000, 100), save_path=\"./outputs/data/raw_data_sim.npy\"):\n",
    "    np.random.seed(seed)\n",
    "    data = np.random.randn(*size)\n",
    "    np.save(save_path, data)\n",
    "    return data\n",
    "\n",
    "# Run all steps of the code/analysis\n",
    "def main():\n",
    "    # Set up output directories\n",
    "    paths = [\"./outputs/\", \"./outputs/data\", \"./outputs/graphics\", \"./outputs/models\"]\n",
    "    create_output_paths(paths)\n",
    "\n",
    "    # Simulate data\n",
    "    data = simulate_data()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ef846",
   "metadata": {},
   "source": [
    "Subsequently, we need to write respective `unit tests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f206c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/test_path_sim_data_gen.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_path_sim_data_gen.py\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pytest\n",
    "from emd_clust_cont_loss_sim_functions import create_output_paths, simulate_data\n",
    "\n",
    "# Define temporary test paths to avoid overwriting real data\n",
    "test_paths = [\"./test_outputs/\", \"./test_outputs/data\", \"./test_outputs/graphics\", \"./test_outputs/models\"]\n",
    "\n",
    "\n",
    "def test_create_output_paths():\n",
    "    # Run the path creation function\n",
    "    create_output_paths(test_paths)\n",
    "    \n",
    "    # Check if each path now exists\n",
    "    for path in test_paths:\n",
    "        assert os.path.isdir(path), f\"Expected directory {path} to be created, but it was not.\"\n",
    "\n",
    "\n",
    "def test_simulate_data():\n",
    "    # Generate test data\n",
    "    data = simulate_data(size=(100, 50), save_path=\"./test_outputs/data/raw_data_sim.npy\")\n",
    "    \n",
    "    # Check type and shape\n",
    "    assert isinstance(data, np.ndarray), \"Expected output type np.ndarray\"\n",
    "    assert data.shape == (100, 50), \"Expected output shape (100, 50)\"\n",
    "\n",
    "    os.remove(\"./test_outputs/data/raw_data_sim.npy\")\n",
    "\n",
    "\n",
    "# Cleanup - remove generated outputs to keep the test environment clean\n",
    "for path in reversed(test_paths):\n",
    "        if os.path.isdir(path):\n",
    "            shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1f875",
   "metadata": {},
   "source": [
    "Using our, by now, well-know `pytest`, we run the `unit tests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2defc6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "testpaths: ./tests\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 8 items\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 12%]\u001b[0m\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[32m                                   [ 25%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 37%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[2-3-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 50%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[3-3-6] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 62%]\u001b[0m\n",
      "tests/test_path_sim_data_gen.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[32m         [ 75%]\u001b[0m\n",
      "tests/test_path_sim_data_gen.py::test_simulate_data \u001b[32mPASSED\u001b[0m\u001b[32m               [ 87%]\u001b[0m\n",
      "tests/test_paths.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[32m                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651d279-db44-4843-b768-af7f8ba20917",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Unit Testing Tips\n",
    "\n",
    "- many `testing frameworks` have `tools` specifically geared towards writing and running unit tests, `pytest` does as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795f758-664d-4811-9630-16ddf5edbc55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- isolate the `development environment` from the `test environment`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c838e41-d319-4d53-aa08-681da17e1203",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- write `test cases` that are independent of each other. For example, if a `unit A` utilises the result supplied by another `unit B`, you should `test` `unit A` with a `test double`, rather than actually calling the `unit B`. If you don’t do this your `test` `failing` may be due to a fault in either `unit A` or `unit B`, making the `bug` harder to trace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6facb0-80a2-40f2-ba01-c78cdcc7b50b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- aim at `covering` all `paths` through a `unit`, pay particular attention to `loop conditions`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc632b1-30ac-496c-a00a-66bb0685ccd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- in addition to `writing cases` to verify the behaviour, write `cases` to ensure the `performance` of the `code`. For example, if a `function` that is supposed to add `two numbers` takes several minutes to run there is likely a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ac472-0ac9-4807-853c-5f2dfa9170f2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- if you find a defect in your `code` write a `test` that exposes it. Why? First, you will later be able to catch the defect if you do not fix it properly. Second, your `test suite` is now more comprehensive. Third, you will most probably be too lazy to write the `test` after you have already fixed the defect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa7fb2-c235-4965-83cb-d2bc9225e1af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Integration tests\n",
    "\n",
    "A level of `software testing` where `individual units` are combined and `tested` as a `group`. The purpose of this level of `testing` is to expose faults in the interaction between integrated `units`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e780cd-15f7-4623-afde-d9849c67f880",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Integration Testing Approaches\n",
    "\n",
    "There are several different approaches to `integration testing`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9115db2-28c9-412d-a032-454590c6392f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `Big Bang`: an approach to `integration testing` where all or most of the `units` are combined together and tested at one go. This approach is taken when the `testing team` receives the entire `software` in a bundle. So what is the difference between `Big Bang` `integration testing` and `system testing`? Well, the former `tests` only the `interactions` between the `units` while the latter `tests` the entire system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c7b98-0bec-444e-837f-3e01795256e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Top Down`: an approach to `integration testing` where `top-level sections` of the `code` (that themselves contain many smaller `units`) are `tested first` and `lower level units` are `tested` step by step after that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fd5b7-bbb4-4102-8574-1424d86b6478",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Bottom Up`: an approach to `integration testing` where `integration` between bottom level sections are tested first and upper-level sections step by step after that. Again `test stubs` should be used, in this case to `simulate` `inputs` from higher level sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9a4eb-e2f4-441f-a9dd-9557635af643",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Sandwich`/`Hybrid` is an approach to `integration testing` which is a combination of `Top Down` and `Bottom Up` approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ccf82b-3113-463e-ab16-eac868f88632",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Which approach you should use will depend on which best suits the nature/structure of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7747a",
   "metadata": {},
   "source": [
    "Based on our `code`/`analyses`, we could test a sequence of steps from `data generation`, `model training`, and `output generation`.\n",
    "\n",
    "1. `End-to-End Path` and `Directory` Creation: Verify that `create_output_paths` sets up all necessary `directories`, and these `directories` are populated with `files` or `data` generated by the pipeline.\n",
    "\n",
    "2. `Data Generation` to `Model Training`: `Simulate data`, `split` it, and ensure that the `model` can be `trained` on this `data`.\n",
    "\n",
    "3. `Model Training` with `Output Verification`: Run a short `training` session and verify that the `loss` decreases over a few `epochs`, indicating the `model` is `learning`.\n",
    "\n",
    "The first step, as always, is to adapt our `code`/`analyses` so that all needed aspects are run in via `functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16d54fdc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./emd_clust_cont_loss_sim_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./emd_clust_cont_loss_sim_functions.py\n",
    "\n",
    "\n",
    "# Import all necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define utility function to create paths\n",
    "def create_output_paths(paths):\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    return paths\n",
    "\n",
    "# Define utility function to generate data\n",
    "def simulate_data(seed=42, size=(2000, 100), save_path=\"./outputs/data/raw_data_sim.npy\"):\n",
    "    np.random.seed(seed)\n",
    "    data = np.random.randn(*size)\n",
    "    np.save(save_path, data)\n",
    "    return data\n",
    "\n",
    "    \n",
    "# Define class for the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define class for the MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define class for the contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = (output1 - output2).pow(2).sum(1)\n",
    "        return torch.mean((1 - label) * euclidean_distance + label * torch.clamp(self.margin - euclidean_distance, min=0.0))\n",
    "\n",
    "\n",
    "# Define utility function to train the encoder\n",
    "def train_encoder(encoder, data_train, epochs=4000, lr=0.001, momentum=0.9):\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.SGD(encoder.parameters(), lr=lr, momentum=momentum)\n",
    "    losses = []\n",
    "    representations_during_training = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = encoder(data_train)\n",
    "        loss = loss_function(outputs, data_train[:, :encoder.layers[-1].out_features])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Save representations every 1000 epochs\n",
    "        if epoch % 1000 == 0:\n",
    "            with torch.no_grad():\n",
    "                representations_during_training.append(encoder(data_train).cpu().numpy())\n",
    "\n",
    "    return losses, representations_during_training\n",
    "\n",
    "\n",
    "# Run all steps of the code/analysis\n",
    "def main():\n",
    "    # Set up output directories\n",
    "    paths = [\"./outputs/\", \"./outputs/data\", \"./outputs/graphics\", \"./outputs/models\"]\n",
    "    create_output_paths(paths)\n",
    "\n",
    "    # Simulate data\n",
    "    data = simulate_data()\n",
    "\n",
    "    # Train Encoder\n",
    "    data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    data_train_torch = torch.from_numpy(data_train).float()\n",
    "    encoder = Encoder(input_dim=100, hidden_dim=100, embedding_dim=50)\n",
    "    losses, representations_during_training = train_encoder(encoder, data_train_torch)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a31234",
   "metadata": {},
   "source": [
    "Next, we need to write `tests` that evaluate the included parts of the `code`/`analyses` back-to-back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70dd7a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/test_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_pipeline.py\n",
    "\n",
    "\n",
    "# Import all necessary modules\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from emd_clust_cont_loss_sim_functions import create_output_paths, simulate_data, Encoder, train_encoder\n",
    "\n",
    "# Define list of paths\n",
    "test_paths = [\"./test_outputs/\", \"./test_outputs/data\", \"./test_outputs/graphics\", \"./test_outputs/models\"]\n",
    "\n",
    "\n",
    "# Define the tests\n",
    "def test_end_to_end_pipeline():\n",
    "    # Step 1: Set up output directories\n",
    "    create_output_paths(test_paths)\n",
    "    for path in test_paths:\n",
    "        assert os.path.isdir(path), f\"Directory {path} was not created.\"\n",
    "\n",
    "    # Step 2: Generate data\n",
    "    data = simulate_data(size=(100, 50), save_path=\"./test_outputs/data/raw_data_sim.npy\")\n",
    "    assert isinstance(data, np.ndarray), \"Expected output type np.ndarray\"\n",
    "    assert data.shape == (100, 50), \"Expected output shape (100, 50)\"\n",
    "    \n",
    "    # Step 3: Split data and prepare for model training\n",
    "    data_train = torch.from_numpy(data[:80]).float()\n",
    "    \n",
    "    # Step 4: Initialize and train the Encoder model for a few epochs\n",
    "    encoder = Encoder(input_dim=50, hidden_dim=25, embedding_dim=10)\n",
    "    losses, _ = train_encoder(encoder, data_train, epochs=10)  # Limit epochs for a quick test\n",
    "    \n",
    "    # Check that the losses are recorded and decrease over epochs\n",
    "    assert len(losses) == 10, \"Expected 10 loss values, one for each epoch.\"\n",
    "    assert losses[-1] < losses[0], \"Expected the loss to decrease over training.\"\n",
    "    \n",
    "\n",
    "# Cleanup - remove generated outputs to keep the test environment clean\n",
    "for path in reversed(test_paths):\n",
    "        if os.path.isdir(path):\n",
    "            shutil.rmtree(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9500c",
   "metadata": {},
   "source": [
    "With that , we are already back to running out `tests`, this time including a rather complex and holistic `integration test`. Please note, that `tests` might start taking longer, as we actually running, ie `testing` quite a bite of `code`, including `model` `training` and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ae48190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "testpaths: ./tests\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 9 items\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 11%]\u001b[0m\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[32m                                   [ 22%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 33%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[2-3-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 44%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[3-3-6] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 55%]\u001b[0m\n",
      "tests/test_path_sim_data_gen.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[32m         [ 66%]\u001b[0m\n",
      "tests/test_path_sim_data_gen.py::test_simulate_data \u001b[32mPASSED\u001b[0m\u001b[32m               [ 77%]\u001b[0m\n",
      "tests/test_paths.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 88%]\u001b[0m\n",
      "tests/test_pipeline.py::test_end_to_end_pipeline \u001b[32mPASSED\u001b[0m\u001b[32m                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m9 passed\u001b[0m\u001b[32m in 4.33s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782bbd28-2589-4bcf-a4ee-ab92184e755f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Integration Testing Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a37a2d-1ebb-4af5-8ec3-a975d1141330",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ensure that you have a proper `Detail Design document` where `interactions` between each `unit` are clearly defined. It is difficult or impossible to perform `integration testing` without this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52926172-ef63-40da-babd-c93f6d52fc31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Make sure that each `unit` is `unit tested` and `fix` any `bugs` before you start `integration testing`. If there is a `bug` in the individual `units` then the `integration tests` will almost certainly `fail` even if there is no `error` in how they are `integrated`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f1b4a-0271-432a-916c-0fde41d6395a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use `mocking`/`stubs` where appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166b798-de1d-4938-8f5e-f07480cff434",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### System tests\n",
    "\n",
    "A level of the `software testing` process where a complete, integrated `system` is `tested`. The purpose of this `test` is to evaluate whether the `system` as a whole gives the correct outputs for given inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055dadb-2312-4147-960b-615418ec3cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### System Testing Tips\n",
    "\n",
    "`System tests`, also called `end-to-end tests`, run the program, well, from end to end. As such these are the most time consuming `tests` to run. Therefore you should only run these if all the `lower-level tests` (`smoke`, `unit`, `integration`) have already passed. If they haven’t, `fix` the `issues` they have detected first before wasting time running `system tests`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a841bf5-25ca-4323-b781-3ca9127c306e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because of their time-consuming nature it will also often be impractical to have enough `system tests` to trace every possible route through a program, especially if there are a significant number of `conditional statements`. Therefore you should consider the `system test` `cases` you run carefully and prioritise:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0bac9-946f-420f-9978-b3e06b936bb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the most common routes through a program\n",
    "\n",
    "- the most important routes for a program\n",
    "\n",
    "- `cases` that are prone to breakage due to structural problems within the program. Though ideally it’s better to just `fix` those problems, but `cases` exist where this may not be feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a27c0-49bb-4604-88f7-8171e6f02c0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because `system tests` can be time consuming it may be impractical to `run` them very regularly (such as multiple times a day after small changes in the `code`). Therefore it can be a good idea to `run` them each night (and to `automate` this process) so that if `errors` are introduced that only `system testing` can detect, the developer(s) will be made aware of them relatively quickly.\n",
    "\n",
    "In order to run a `system test` for our `code`/`analyses`, we need to refactor the entire `script` to utilize `functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f80a6a8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./emd_clust_cont_loss_sim_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./emd_clust_cont_loss_sim_functions.py\n",
    "\n",
    "\n",
    "# Import necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define utility function to create paths\n",
    "def create_output_paths(paths):\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    return paths\n",
    "\n",
    "# Define utility function to simulate data\n",
    "def simulate_data(seed=42, size=(2000, 100), save_path=\"./outputs/data/raw_data_sim.npy\"):\n",
    "    np.random.seed(seed)\n",
    "    data = np.random.randn(*size)\n",
    "    np.save(save_path, data)\n",
    "    return data\n",
    "\n",
    "# Define utility function to data plots\n",
    "def plot_data_samples(data, save_path=\"./outputs/graphics/data_examples.png\"):\n",
    "    fig, axes = plt.subplots(5, 1, sharex=True, figsize=(16, 10))\n",
    "    for sample, ax in zip(np.arange(5), axes.flat):\n",
    "        sns.heatmap(data[sample].reshape(-1, 100), cbar=False, annot=False,\n",
    "                    xticklabels=False, yticklabels=False, ax=ax)\n",
    "        ax.set_title(f'data sample {sample+1}')\n",
    "    fig.savefig(save_path)\n",
    "\n",
    "\n",
    "# Define class for the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Define class for the MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Define class for the contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = (output1 - output2).pow(2).sum(1)\n",
    "        return torch.mean((1 - label) * euclidean_distance + label * torch.clamp(self.margin - euclidean_distance, min=0.0))\n",
    "\n",
    "\n",
    "# Define utility function to train the encoder\n",
    "def train_encoder(encoder, data_train, epochs=4000, lr=0.001, momentum=0.9):\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.SGD(encoder.parameters(), lr=lr, momentum=momentum)\n",
    "    losses = []\n",
    "    representations_during_training = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = encoder(data_train)\n",
    "        loss = loss_function(outputs, data_train[:, :encoder.layers[-1].out_features])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Save representations every 1000 epochs\n",
    "        if epoch % 1000 == 0:\n",
    "            with torch.no_grad():\n",
    "                representations_during_training.append(encoder(data_train).cpu().numpy())\n",
    "\n",
    "    return losses, representations_during_training\n",
    "\n",
    "\n",
    "# Define utility function to plot loss\n",
    "def plot_loss(losses, title, save_path):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.lineplot(losses, ax=ax)\n",
    "    sns.despine(offset=10, ax=ax)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch number\")\n",
    "    plt.ylabel(\"Training loss\")\n",
    "    fig.savefig(save_path)\n",
    "\n",
    "\n",
    "# Define utility function to plot encoder representations\n",
    "def plot_encoder_representations(representations, save_path):\n",
    "    fig, axes = plt.subplots(1, 5, sharex=True, figsize=(10, 2))\n",
    "    for sample, ax in zip(np.arange(5), axes.flat):\n",
    "        sns.heatmap(representations[sample].reshape(-1, 5), cbar=False, annot=False,\n",
    "                    xticklabels=False, yticklabels=False, ax=ax)\n",
    "        ax.set_title(f'Sample {sample+1}')\n",
    "    fig.savefig(save_path)\n",
    "\n",
    "\n",
    "# Define utility function to apply clustering\n",
    "def apply_clustering(data, save_path='./outputs/models/dpgmm.joblib', n_components=10):\n",
    "    dpgmm = BayesianGaussianMixture(\n",
    "        n_components=n_components, covariance_type='full',\n",
    "        weight_concentration_prior_type=\"dirichlet_process\",\n",
    "        weight_concentration_prior=0.1, random_state=42)\n",
    "    dpgmm.fit(data)\n",
    "    dump(dpgmm, save_path)\n",
    "    return dpgmm\n",
    "\n",
    "\n",
    "# Define utility function to plot TSNE\n",
    "def plot_tsne(data, labels, save_path):\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(data)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['t-SNE dim 1'] = tsne_results[:, 0]\n",
    "    df['t-SNE dim 2'] = tsne_results[:, 1]\n",
    "    df['cluster'] = labels\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    sns.scatterplot(x=\"t-SNE dim 1\", y=\"t-SNE dim 2\", palette=sns.color_palette(\"hls\", len(set(labels))),\n",
    "                    data=df, hue=\"cluster\", legend=\"full\", alpha=0.3)\n",
    "    sns.despine(offset=10)\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "\n",
    "# Define analysis steps\n",
    "def main():\n",
    "    # Set up output directories\n",
    "    paths = [\"./outputs/\", \"./outputs/data\", \"./outputs/graphics\", \"./outputs/models\"]\n",
    "    create_output_paths(paths)\n",
    "\n",
    "    # Simulate data\n",
    "    data = simulate_data()\n",
    "\n",
    "    # Plot data samples\n",
    "    plot_data_samples(data)\n",
    "\n",
    "    # Train Encoder\n",
    "    data_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    data_train_torch = torch.from_numpy(data_train).float()\n",
    "    encoder = Encoder(input_dim=100, hidden_dim=100, embedding_dim=50)\n",
    "    losses, representations_during_training = train_encoder(encoder, data_train_torch)\n",
    "    torch.save(encoder, './outputs/models/encoder.pth')\n",
    "    plot_loss(losses, \"Loss of Encoder\", './outputs/graphics/loss_training.png')\n",
    "\n",
    "    # Plot encoder representations at epoch 3000\n",
    "    plot_encoder_representations(representations_during_training[3], './outputs/graphics/data_representations_examples.png')\n",
    "\n",
    "    # Apply clustering\n",
    "    dpgmm = apply_clustering(representations_during_training[3])\n",
    "    cluster_assignments_train = dpgmm.predict(representations_during_training[3])\n",
    "\n",
    "    # t-SNE visualization of cluster assignments in training data\n",
    "    plot_tsne(representations_during_training[3], cluster_assignments_train, './outputs/graphics/tsne_rep_clust_train.png')\n",
    "\n",
    "    # Apply trained encoder to test data\n",
    "    data_test_torch = torch.from_numpy(data_test).float()\n",
    "    encoder_embeddings_test = encoder(data_test_torch).detach().numpy()\n",
    "\n",
    "    # Predict clusters for test data\n",
    "    cluster_assignments_test = dpgmm.predict(encoder_embeddings_test)\n",
    "\n",
    "    # t-SNE visualization of test data clusters\n",
    "    plot_tsne(encoder_embeddings_test, cluster_assignments_test, './outputs/graphics/tsne_rep_clust_test.png')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de91c42",
   "metadata": {},
   "source": [
    "Now to the second part: writing a `system level test`. Here's what it should do:\n",
    "\n",
    "1. The `script` runs `end-to-end` without errors, simulating a full workflow execution.\n",
    "\n",
    "2. `Output directories` and `files` are created as expected, confirming each step’s success.\n",
    "\n",
    "3. `Model` behavior is evaluated by checking if it can produce `embeddings` and if `training results` in a `reduction` of `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a29afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/test_system_analyses.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_system_analyses.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from emd_clust_cont_loss_sim_functions import main, Encoder, simulate_data, train_encoder\n",
    "\n",
    "def test_system_workflow():\n",
    "    # Set up paths for verification\n",
    "    output_paths = [\"./outputs/\", \"./outputs/data\", \"./outputs/graphics\", \"./outputs/models\"]\n",
    "    data_file = \"./outputs/data/raw_data_sim.npy\"\n",
    "    plot_file = \"./outputs/graphics/data_examples.png\"\n",
    "    model_file = \"./outputs/models/encoder.pth\"\n",
    "    \n",
    "    # Run the main function to execute the workflow\n",
    "    main()\n",
    "    \n",
    "    # Check that output directories are created\n",
    "    for path in output_paths:\n",
    "        assert os.path.isdir(path), f\"Expected directory {path} to be created.\"\n",
    "\n",
    "    # Verify data file existence and format\n",
    "    assert os.path.isfile(data_file), \"Expected data file not found.\"\n",
    "    data = np.load(data_file)\n",
    "    assert isinstance(data, np.ndarray), \"Data file is not in the expected .npy format.\"\n",
    "    \n",
    "    # Verify the plot file is created\n",
    "    assert os.path.isfile(plot_file), \"Expected plot file not found.\"\n",
    "\n",
    "    # Verify the model file is created\n",
    "    assert os.path.isfile(model_file), \"Expected model file not found.\"\n",
    "    \n",
    "    # Step 1: Load and check model\n",
    "    encoder = torch.load(model_file)\n",
    "    assert isinstance(encoder, Encoder), \"The loaded model is not an instance of the Encoder class.\"\n",
    "    \n",
    "    # Step 2: Evaluate model with sample data\n",
    "    # Use a small subset of the data to generate embeddings and verify the output shape\n",
    "    data_sample = torch.from_numpy(data[:10]).float()  # Take a sample of 10 for testing\n",
    "    with torch.no_grad():\n",
    "        embeddings = encoder(data_sample)\n",
    "    \n",
    "    # Check the output embeddings shape\n",
    "    expected_shape = (10, encoder.layers[-1].out_features)\n",
    "    assert embeddings.shape == expected_shape, f\"Expected embeddings shape {expected_shape}, but got {embeddings.shape}.\"\n",
    "\n",
    "    # Step 3: Check if training reduces loss over epochs\n",
    "    # Train the model for a small number of epochs and confirm that loss decreases\n",
    "    data_train = torch.from_numpy(data[:80]).float()\n",
    "    initial_loss, final_loss = None, None\n",
    "    encoder = Encoder(input_dim=100, hidden_dim=100, embedding_dim=50)  # Reinitialize the encoder for testing\n",
    "    losses, _ = train_encoder(encoder, data_train, epochs=10)  # Train for a few epochs for quick testing\n",
    "    \n",
    "    # Capture initial and final losses\n",
    "    initial_loss, final_loss = losses[0], losses[-1]\n",
    "    \n",
    "    # Verify that the final loss is lower than the initial loss\n",
    "    assert final_loss < initial_loss, \"Expected final loss to be lower than initial loss, indicating training progress.\"\n",
    "\n",
    "    # Cleanup - remove generated outputs to keep the test environment clean\n",
    "    for path in reversed(output_paths):\n",
    "            if os.path.isdir(path):\n",
    "                shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a85e6",
   "metadata": {},
   "source": [
    "The moment of truth is here: we run the `tests` as usual and will grab something to drink/snack, as this might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a6bd58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "testpaths: ./tests\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 10 items\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[33m                              [ 10%]\u001b[0m\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[33m                                   [ 20%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[33m                 [ 30%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[2-3-5] \u001b[32mPASSED\u001b[0m\u001b[33m                 [ 40%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[3-3-6] \u001b[32mPASSED\u001b[0m\u001b[33m                 [ 50%]\u001b[0m\n",
      "tests/test_path_sim_data_gen.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[33m         [ 60%]\u001b[0m\n",
      "tests/test_path_sim_data_gen.py::test_simulate_data \u001b[32mPASSED\u001b[0m\u001b[33m               [ 70%]\u001b[0m\n",
      "tests/test_paths.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[33m                     [ 80%]\u001b[0m\n",
      "tests/test_pipeline.py::test_end_to_end_pipeline \u001b[32mPASSED\u001b[0m\u001b[33m                  [ 90%]\u001b[0m\n",
      "tests/test_system_analyses.py::test_system_workflow \u001b[32mPASSED\u001b[0m\u001b[33m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/utils.py:11\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/utils.py:11: DeprecationWarning: \n",
      "  Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "  (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "  but was not found to be installed on your system.\n",
      "  If this would cause problems for you,\n",
      "  please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "          \n",
      "    import pandas as pd\n",
      "\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/rcmod.py:82\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "    if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/setuptools/_distutils/version.py:346\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "    other = LooseVersion(other)\n",
      "\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.\n",
      "    mpl_cm.register_cmap(_name, _cmap)\n",
      "\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.\n",
      "    mpl_cm.register_cmap(_name + \"_r\", _cmap_r)\n",
      "\n",
      "tests/test_system_analyses.py::test_system_workflow\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "    warnings.warn(\n",
      "\n",
      "tests/test_system_analyses.py::test_system_workflow\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "  Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "  the same time. Both libraries are known to be incompatible and this\n",
      "  can cause random crashes or deadlocks on Linux when loaded in the\n",
      "  same Python program.\n",
      "  Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "  information and possible workarounds, please see\n",
      "      https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "  \n",
      "    warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================= \u001b[32m10 passed\u001b[0m, \u001b[33m\u001b[1m17 warnings\u001b[0m\u001b[33m in 16.15s\u001b[0m\u001b[33m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46580e9-abbc-4d6d-a924-115869dbf977",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Acceptance and regression tests\n",
    "\n",
    "A level of the `software testing process` where a `system` is `tested` for acceptability. The purpose of this `test` is to evaluate the `system`’s compliance with the `project` requirements and assess whether it is acceptable for the purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c881d-8bb4-4f59-8ebf-5df943967df8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Acceptance testing\n",
    "\n",
    "`Acceptance tests` are one of the last `tests types` that are performed on `software` prior to delivery. `Acceptance testing` is used to determine whether a piece of `software` satisfies all of the requirements from user’s perspective. Does this piece of `software` do what it needs to do? These `tests` are sometimes built against the original specification.\n",
    "\n",
    "Because `research software` is typically written by the researcher that will use it (or at least with significant `input` from them) `acceptance tests` may not be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470f0bd-55e4-480a-9055-121cbf75a846",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Regression testing\n",
    "\n",
    "`Regression testing` checks for unintended changes by comparing new test results to previous ones, ensuring updates don't break the `software`. It's critical because even unrelated `code` changes can cause issues. Suitable for all `testing levels`, it's vital in `system testing` and can automate tedious manual `checks`. `Tests` are created by recording `outputs` for specific `inputs`, then `retesting` and comparing `results` to detect discrepancies. Essential for team projects, it's also crucial for solo work to catch self-introduced `errors`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa261d3-9948-437c-a441-fe37f1f10a05",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`Regression testing` approaches differ in their focus. Common examples include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30bb75-230d-4cd1-9aea-93fa9eaaf4cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Bug regression`: retest a specific `bug` that has been allegedly `fixed`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb612b-8d0f-49c5-a502-9bb2787d5903",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Old fix regression testing`: retest several old `bugs` that were `fixed`, to see if they are back. (This is the classical notion of `regression`: the program has regressed to a bad state.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ccff36-49fd-41be-b3bb-d94c32b6ee79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `General functional regression`: `retest` the project broadly, including areas that worked before, to see whether more recent changes have destabilized working `code`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f33b54-0bd0-4cb9-aecd-f75f4b7c693d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `Conversion or port testing`: the program is `ported` to a new platform and a `regression test suite` is run to determine whether the `port` was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1e332-d30d-4a25-92ca-baf6157001e9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `Configuration testing`: the `program` is `run` with a new `device` or on a new `version` of the `operating system` or in conjunction with a new application. This is like `port testing` except that the underlying `code` hasn’t been changed–only the external `components` that the `software` under `test` must interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb63ea",
   "metadata": {},
   "source": [
    "As we already refactored the entire `code`/`analyses` to utilize ``functions`, we can start writing a set of regression `tests` right away. In more detail we will ensure that the generated `data`, `model training`, and `model output` remain consistent:\n",
    "\n",
    "1. `Check Data Consistency`: Verify that the `data` generated by `simulate_data` has a `specific shape` and `sample values`.\n",
    "\n",
    "2. `Check Loss Consistency`: `Train` the `Encoder` for a few `epochs` and confirm that the final `loss` falls within an `expected range`, ensuring the `model` `trains` as expected.\n",
    "\n",
    "3. `Check Model Output Consistency`: Confirm that the output `embeddings` have the correct `shape` and that specific `values` in the output are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92d499d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tests/test_regeression_analyses.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tests/test_regression_analyses.py\n",
    "\n",
    "# Import all necessary modules\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from emd_clust_cont_loss_sim_functions import simulate_data, Encoder, train_encoder\n",
    "\n",
    "\n",
    "# Define list of paths\n",
    "test_paths = [\"./test_outputs/\", \"./test_outputs/data\", \"./test_outputs/graphics\", \"./test_outputs/models\"]\n",
    "\n",
    "\n",
    "# Define regression test\n",
    "def test_regression_workflow():\n",
    "    # Part 1: Data Consistency Check\n",
    "    # Generate test data and check shape and sample values\n",
    "    data = simulate_data(size=(100, 100), save_path=\"./test_outputs/data/raw_data_sim.npy\")\n",
    "      \n",
    "    # Verify data shape\n",
    "    assert data.shape == (100, 100), \"Data shape mismatch.\"\n",
    "\n",
    "    # Part 2: Loss Consistency Check\n",
    "    # Initialize the encoder and train it briefly, then check final loss\n",
    "    input_dim = 100\n",
    "    encoder = Encoder(input_dim=input_dim, hidden_dim=100, embedding_dim=50)\n",
    "    data_train = torch.from_numpy(data).float()\n",
    "\n",
    "    # Train the encoder for a few epochs\n",
    "    losses, _ = train_encoder(encoder, data_train, epochs=10)\n",
    "\n",
    "    # Check that the final loss is within the expected range\n",
    "    final_loss = losses[-1]\n",
    "    assert 0.01 < final_loss < 2.0, f\"Final loss {final_loss} is outside the expected range (0.01, 2.0).\"\n",
    "\n",
    "    # Part 3: Model Output Consistency Check\n",
    "    # Generate embeddings and verify their shape and sample values\n",
    "    data_sample = torch.from_numpy(data[:10]).float()  # Take a sample of 10 rows for testing\n",
    "    with torch.no_grad():\n",
    "        embeddings = encoder(data_sample)\n",
    "\n",
    "    # Verify output shape\n",
    "    assert embeddings.shape == (10, 50), \"Embedding shape mismatch.\"\n",
    "\n",
    "    # Expected embedding sample values (replace with known values for regression testing)\n",
    "    expected_embedding_sample = embeddings[0].numpy()[:5]  # Capture this once and use it as reference\n",
    "    np.testing.assert_almost_equal(embeddings[0].numpy()[:5], expected_embedding_sample, decimal=6,\n",
    "                                   err_msg=\"Embedding does not match expected sample values.\")\n",
    "\n",
    "# Cleanup - remove generated outputs to keep the test environment clean\n",
    "for path in reversed(test_paths):\n",
    "        if os.path.isdir(path):\n",
    "            shutil.rmtree(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c9706",
   "metadata": {},
   "source": [
    "For the last time in this session, let's run the `tests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "528a932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-8.3.3, pluggy-1.5.0 -- /Users/peerherholz/anaconda3/envs/course_name/bin/python3.1\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/peerherholz/google_drive/GitHub/Clean_Repro_Code_Neuromatch/workshop/materials/code_form_test_CI\n",
      "configfile: pytest.ini\n",
      "testpaths: ./tests\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 11 items\n",
      "\n",
      "tests/test_addition.py::test_example \u001b[32mPASSED\u001b[0m\u001b[33m                              [  9%]\u001b[0m\n",
      "tests/test_fixture.py::test_sum \u001b[32mPASSED\u001b[0m\u001b[33m                                   [ 18%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[1-1-2] \u001b[32mPASSED\u001b[0m\u001b[33m                 [ 27%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[2-3-5] \u001b[32mPASSED\u001b[0m\u001b[33m                 [ 36%]\u001b[0m\n",
      "tests/test_parameterized.py::test_addition[3-3-6] \u001b[32mPASSED\u001b[0m\u001b[33m                 [ 45%]\u001b[0m\n",
      "tests/test_path_sim_data_gen.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[33m         [ 54%]\u001b[0m\n",
      "tests/test_path_sim_data_gen.py::test_simulate_data \u001b[32mPASSED\u001b[0m\u001b[33m               [ 63%]\u001b[0m\n",
      "tests/test_paths.py::test_create_output_paths \u001b[32mPASSED\u001b[0m\u001b[33m                     [ 72%]\u001b[0m\n",
      "tests/test_pipeline.py::test_end_to_end_pipeline \u001b[32mPASSED\u001b[0m\u001b[33m                  [ 81%]\u001b[0m\n",
      "tests/test_regeression_analyses.py::test_regression_workflow \u001b[32mPASSED\u001b[0m\u001b[33m      [ 90%]\u001b[0m\n",
      "tests/test_system_analyses.py::test_system_workflow \u001b[32mPASSED\u001b[0m\u001b[33m               [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/utils.py:11\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/utils.py:11: DeprecationWarning: \n",
      "  Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "  (to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "  but was not found to be installed on your system.\n",
      "  If this would cause problems for you,\n",
      "  please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "          \n",
      "    import pandas as pd\n",
      "\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/rcmod.py:82\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "    if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/setuptools/_distutils/version.py:346\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "    other = LooseVersion(other)\n",
      "\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1582: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.\n",
      "    mpl_cm.register_cmap(_name, _cmap)\n",
      "\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "../../../../../../anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/cm.py:1583: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.\n",
      "    mpl_cm.register_cmap(_name + \"_r\", _cmap_r)\n",
      "\n",
      "tests/test_system_analyses.py::test_system_workflow\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "    warnings.warn(\n",
      "\n",
      "tests/test_system_analyses.py::test_system_workflow\n",
      "  /Users/peerherholz/anaconda3/envs/course_name/lib/python3.10/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "  Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "  the same time. Both libraries are known to be incompatible and this\n",
      "  can cause random crashes or deadlocks on Linux when loaded in the\n",
      "  same Python program.\n",
      "  Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "  information and possible workarounds, please see\n",
      "      https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "  \n",
      "    warnings.warn(msg, RuntimeWarning)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================= \u001b[32m11 passed\u001b[0m, \u001b[33m\u001b[1m17 warnings\u001b[0m\u001b[33m in 16.76s\u001b[0m\u001b[33m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893e13f-a62d-43ca-b138-e7392eed9063",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion:\n",
    "\n",
    "Embracing `testing` and `testing frameworks` like `pytest` and incorporating a comprehensive `testing strategy` are essential steps towards achieving high-quality software development. These `frameworks` not only `automate` the `testing` process but also provide a structured approach to addressing a wide spectrum of `testing` requirements. By leveraging their capabilities, `researchers` and `software developers` can ensure thorough `test coverage`, streamline `debugging`, and maintain high standards of `software quality` and `performance``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d99673",
   "metadata": {},
   "source": [
    "```{admonition} Task for y'all!\n",
    "\n",
    "Remember our `script` from the beginning? You already went through it a couple of times and brought to `code` (get it?). Now, we would like to add some `tests` for our `script` to ensure its functionality.\n",
    "\n",
    "\n",
    "You have 40 min.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "clean_repro_code_nm",
   "language": "python",
   "name": "clean_repro_code_nm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
